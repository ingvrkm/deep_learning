หวั ขอ้ final project
การจําแนกอารมณข์ องขอ้ ความรวี วิ ภาพยนตร ์(Sentiment Analysis of Movie
Reviews)
หวั ขอ้ นนี้ า่ สนใจอยา่ งไร ทาํ ไมถงึ เลอื กหวั ขอ้ นมี้ าทาํ เป็น final project
หวั ขอ้ นนี้ ่าสนใจเพราะเป็นการประยกุ ต ์Deep Learning กบั งานประมวลผลภาษา
ธรรมชาต ิ(NLP) ซงึ่ กําลงั เป็นทนี่ ยิ มอยา่ งมาก โดยเฉพาะการจําแนกอารมณข์ องขอ้ ความรวี วิ ที่
สามารถนําไปใชไ้ ดจ้ รงิ ในหลายดา้ น เชน่ การวเิ คราะหค์ วามคดิ เห็นลกู คา้ หรอื ระบบแนะนํา
สนิ คา้ อกี ทงั้ เป็นโจทยท์ ที่ า้ ทาย เพราะตอ้ งใหโ้ มเดลเขา้ ใจอารมณท์ ซี่ อ่ นอยใู่ นขอ้ ความภาษา
องั กฤษ จงึ เลอื กทําหวั ขอ้ นเี้ พอื่ ฝึกสรา้ งและปรับแตง่ โมเดลดว้ ยตนเอง
ทาํ ไมหวั ขอ้ นจี้ งึ ตอ้ งใช ้deep learning ในการแกป้ ญั หา เปรยี บเทยี บกบั การแกป้ ญั หา
นดี้ ว้ ยวธิ อี นื่ ๆ วธิ ีdeep learning มขี อ้ เดน่ ขอ้ ดอ้ ยอยา่ งไร
เพราะปัญหาการจําแนกอารมณข์ องขอ้ ความเป็นงานทซี่ บั ซอ้ นและตอ้ งอาศยั ความ
เขา้ ใจบรบิ ทของภาษา ซงึ่ วธิ แี บบดงั้ เดมิ เชน่ Bag-of-Words หรอื Naive Bayes จะพจิ ารณา
เพยี งจากการนับคํา โดยไมเ่ ขา้ ใจลําดบั หรอื ความสมั พันธข์ องคําในประโยค ทําใหม้ ี
ประสทิ ธภิ าพตํา่ เมอื่ เจอประโยคทมี่ อี ารมณแ์ ฝงหรอื ประชดประชนั
ขอ้ เดน่ ของ Deep Learning:
- สามารถเรยี นรลู้ กั ษณะเชงิ ลกึ และบรบิ ทของภาษาไดด้ ี(เชน่ จาก LSTM หรอื
Transformer)
- ไมต่ อ้ งพงึ่ พาการออกแบบ feature ดว้ ยมอื มาก
- ปรับใชก้ บั ขอ้ ความจํานวนมากไดแ้ ละใหค้ วามแมน่ ยําสงู
ขอ้ ดอ้ ยของ Deep Learning:
- ตอ้ งใชข้ อ้ มลู และพลงั ประมวลผลสงู
- ใชเ้วลาเทรนมากและอาจปรับจนู ยาก
- ทําความเขา้ ใจเชงิ เหตผุ ลของผลลพั ธไ์ ดย้ าก
อธบิ ายสถาปตั ยกรรม deep learning ทใี่ ช ้(feedforward NN CNN RNN GAN หรอื
VAE) วาดรปู แสดงจาํ นวนโหนด weight bias รวมถงึ การเชอื่ มตอ่ และ activation
function ตา่ งๆใหช้ ดั เจน
สถาปัตยกรรมทใี่ ชใ้ นโปรเจคนคี้ อื Recurrent Neural Network (RNN) ประเภท
LSTM (Long Short-Term Memory) ซงึ่ ออกแบบมาเพอื่ จัดการขอ้ มลู แบบลําดบั (sequence
data) เชน่ ขอ้ ความ โดย LSTM สามารถจดจําขอ้ มลู ระยะยาวและป้องกนั ปัญหา gradient
vanishing ทพี่ บใน RNN ทวั่ ไปไดด้ ี
สถาปัตยกรรมของโมเดลประกอบดว้ ย:
1. Embedding Layer : แปลงแตล่ ะคําในรวี วิ ใหเ้ป็นเวกเตอรข์ นาด 128 มติ ิเพอื่ แทน
ความหมายเชงิ ตวั เลขของคํา
2. LSTM Layer : ประมวลผลลําดบั คําในประโยค โดยใช ้hidden units จํานวน 128
หน่วย เพอื่ เรยี นรบู้ รบิ ทของขอ้ ความ
3. Fully Connected (Linear) Layer : รับคา่ จาก hidden state สดุ ทา้ ยของ LSTM แลว้
สง่ ผา่ นไปยงั output layer ขนาด 2 โหนด (แทน Positive / Negative)

4. Activation Function :
a. ใช ้tanh และ sigmoid ภายใน LSTM
b. ใช ้softmax ท ี่output layer เพอื่ แปลงคา่ เป็นความน่าจะเป็นของแตล่ ะคลาส
อธบิ ายโคด้ PyTorch รวมไปถงึ โคด้ สว่ นอนื่ ๆทใี่ ชใ้ นการเชอื่ มตอ่ กบั โมเดลอนื่ ๆ อยา่ ง
ชดั เจน สว่ นไหนจดั การกบั ขอ้ มลู สว่ นไหนสรา้ งโมเดล สว่ นไหน train ฯลฯ
○ สง่ ลงิ คท์ นี่ ําไปสโู่ คด้ PyTorch ทที่ กุ คนเขา้ ถงึ ไดม้ าดว้ ย ควรจะเป็น Github
public repor link
ขนั้ ตอนท ี่1: ตดิ ตงั้ Library ทจี่ ําเป็นสําหรับการทํางานของโมเดล Deep Learning
ในขนั้ ตอนแรกน ี้ จะตดิ ตงั้ ไลบรารหี ลกั ทใี่ ชใ้ นการพัฒนาโมเดลจําแนกอารมณข์ องขอ้ ความรวี วิ
ภาพยนตร ์(Sentiment Analysis) โดยใช ้Deep Learning
- PyTorch: ใชส้ ําหรับสรา้ งโมเดล Neural Network และฝึกสอนโมเดล
- Torchvision / Torchaudio: ใชเ้สรมิ สําหรับจัดการขอ้ มลู ภาพหรอื เสยี ง (แมใ้ นงานนจี้ ะ
ไมใ่ ชโ้ ดยตรง แตจ่ ําเป็นตอ่ การตดิ ตงั้ บางสว่ นของ PyTorch)
- Transformers: ใชส้ ําหรับเรยี กใชง้ าน tokenizer และ pre-trained embeddings จาก
HuggingFace
- Datasets: ใชโ้ หลดและจัดการขอ้ มลู รวี วิ จากแหลง่ ขอ้ มลู มาตรฐาน เชน่ IMDB
การตดิ ตงั้ นที้ ําใหม้ สี ภาพแวดลอ้ มพรอ้ มสําหรับการสรา้ งและฝึกโมเดลในขนั้ ตอนถดั ไป
ขนั้ ตอนท ี่2: นําเขา้ ไลบรารที จี่ ําเป็น (Import Libraries)
หลงั จากตดิ ตงั้ แพ็กเกจทงั้ หมดเรยี บรอ้ ยแลว้ ขนั้ ตอนนคี้ อื การนําเขา้ (import) ไลบรารหี ลกั ทจี่ ะ
ใชต้ ลอดทงั้ โปรเจค โดยแตล่ ะตวั มหี นา้ ทดี่ งั น:ี้
- torch, torch.nn, torch.optim, torch.utils.data → สําหรับสรา้ งและจัดการโมเดล
Neural Network ดว้ ย PyTorch

- torch.nn.functional (F) → ใชง้ านฟังกช์ นั เชน่ activation functions (ReLU,
sigmoid, softmax)
- transformers → ใชส้ ําหรับโหลด tokenizer และ pre-trained models (เชน่ BERT)
- datasets → สําหรับโหลดชดุ ขอ้ มลู IMDB Review
- numpy, random, re → ใชจ้ ัดการขอ้ มลู , ควบคมุ คา่ seed เพอื่ ใหไ้ ดผ้ ลลพั ธซ์ าํ้ ได,้
และจัดการขอ้ ความดว้ ย Regular Expression
การ import ไลบรารที งั้ หมดนเี้ ป็นการเตรยี มเครอื่ งมอื พนื้ ฐานสําหรับการสรา้ งโมเดล deep
learning ในขนั้ ตอนตอ่ ไป
ขนั้ ตอนท ี่3: โหลดและสํารวจขอ้ มลู (Dataset Loading & Exploration)
ในขนั้ ตอนน ี้ จะใชไ้ ลบราร ีHuggingFace Datasets เพอื่ โหลดชดุ ขอ้ มลู มาตรฐานทชี่ อื่ วา่
IMDB Dataset ซงึ่ เป็นชดุ ขอ้ มลู รวี วิ ภาพยนตรท์ ถี่ กู ตดิ ป้ายกํากบั อารมณ ์ (sentiment) ไวแ้ ลว้
วา่
- 0 = รวี วิ เชงิ ลบ (negative)
- 1 = รวี วิ เชงิ บวก (positive)
ชดุ ขอ้ มลู นมี้ ตี วั อยา่ งขอ้ ความขนาดใหญ ่ เหมาะกบั การฝึกสอนโมเดล deep learning สําหรับ
งาน Sentiment Analysis โดยเฉพาะ หลงั จากโหลดขอ้ มลู แลว้ จะลองพมิ พต์ วั อยา่ งขอ้ มลู บาง
สว่ นจากชดุ train เพอื่ ดโู ครงสรา้ งจรงิ ของ dataset กอ่ นเขา้ สขู่ นั้ ตอน preprocessing
ขนั้ ตอนท ี่4: การเตรยี มขอ้ มลู ขอ้ ความ (Text Preprocessing)
ขอ้ ความรวี วิ ในชดุ ขอ้ มลู IMDB มกั มอี กั ขระพเิ ศษ สญั ลกั ษณ ์ หรอื เครอื่ งหมายวรรคตอนทไี่ ม่
จําเป็นตอ่ การเรยี นรขู้ องโมเดล Deep Learning ดงั นัน้ กอ่ นทจี่ ะนําขอ้ มลู เขา้ Tokenizer จําเป็น
ตอ้ งทําการ Preprocessing เพอื่ ทําใหข้ อ้ ความอยใู่ นรปู แบบทสี่ ะอาดและเหมาะสมมากขนึ้ โดย
ในขนั้ ตอนนจี้ ะทําการ:
- แปลงขอ้ ความใหเ้ป็นตวั พมิ พเ์ ล็กทงั้ หมด (lowercase) เพอื่ ใหค้ ําเหมอื นกนั มรี ปู แบบ
เดยี วกนั
- ลบอกั ขระพเิ ศษและสญั ลกั ษณท์ ไี่ มจ่ ําเป็น เชน่ !, ., ?, , เป็นตน้
จากนัน้ ใชค้ ําสงั่ dataset.map() เพอื่ ใหฟ้ ังกช์ นั clean_text() ทํางานกบั ทกุ ตวั อยา่ งในชดุ
ขอ้ มลู อยา่ งมปี ระสทิ ธภิ าพ ผลลพั ธค์ อื ชดุ ขอ้ มลู ทขี่ อ้ ความไดร้ ับการทําความสะอาดเรยี บรอ้ ย
พรอ้ มเขา้ สกู่ ระบวนการ Tokenization ในขนั้ ตอนถดั ไป
ขนั้ ตอนท ี่5: Tokenization และการเตรยี มขอ้ มลู สําหรับโมเดล (Data Preparation)
หลงั จากทที่ ําความสะอาดขอ้ ความเรยี บรอ้ ยแลว้ ขนั้ ตอนตอ่ มาคอื การ แปลงขอ้ ความใหก้ ลาย
เป็นตวั เลข (Tokenization) เพอื่ ใหโ้ มเดล Deep Learning สามารถเรยี นรไู้ ด ้ในทนี่ จี้ ะใช ้
Tokenizer จาก HuggingFace (BERT-base-uncased) ซงึ่ ทําหนา้ ท ี่
- แปลงคําในขอ้ ความใหเ้ป็น token IDs ตามพจนานุกรมของ BERT
- กําหนดความยาวสงู สดุ (max_length=256)
- เตมิ padding เพอื่ ใหค้ วามยาวของทกุ ขอ้ ความเทา่ กนั
- ตดั ขอ้ ความทยี่ าวเกนิ ไป (truncation)
จากนัน้ จะนําผลลพั ธท์ ไี่ ดม้ าสรา้ งเป็น Custom PyTorch Dataset เพอื่ ให ้DataLoader
สามารถสมุ่ ชดุ ขอ้ มลู ทลี ะ batch สําหรับใชฝ้ ึกสอนโมเดลไดส้ ะดวก ขนั้ ตอนนถี้ อื เป็นการเตรยี ม
ขอ้ มลู เขา้ สกู่ ระบวนการ Deep Learning อยา่ งสมบรู ณ ์
ขนั้ ตอนท ี่6: ปรับปรงุ โครงสรา้ ง Dataset สําหรับ PyTorch (Custom Dataset Class)

หลงั จากทที่ ําการ Tokenize ชดุ ขอ้ มลู เรยี บรอ้ ยแลว้ ผลลพั ธท์ ไี่ ดจ้ าก HuggingFace จะอยใู่ น
รปู แบบของ dictionary ทมี่ ีkey เชน่ input_ids, attention_mask, และ label ซงึ่ ยงั ไมส่ ามารถ
นําไปใชก้ บั PyTorch ไดโ้ ดยตรง
ในขนั้ ตอนน ี้ จะสรา้ งคลาส IMDBDataset ขนึ้ มาใหม ่ เพอื่ แปลงขอ้ มลู เหลา่ นใี้ หก้ ลาย
เป็น PyTorch Tensor ทสี่ ามารถโหลดเขา้ สโู่ มเดลผา่ น DataLoader ไดอ้ ยา่ งมปี ระสทิ ธภิ าพ
การออกแบบคลาส Dataset เองทําใหเ้ราสามารถควบคมุ รปู แบบของขอ้ มลู ทสี่ ง่ เขา้ โม
เดลได ้และเป็นพนื้ ฐานสําคญั ของการเทรนโมเดล deep learning ดว้ ย PyTorch
ขนั้ ตอนท ี่7: ออกแบบโมเดล Deep Learning (LSTM Classifier)
ในขนั้ ตอนน ี้ จะสรา้ งโมเดลแบบ Recurrent Neural Network (RNN) โดยใช ้Long
Short-Term Memory (LSTM) ซงึ่ เป็นสถาปัตยกรรมทเี่ หมาะสมสําหรับการประมวลผลขอ้ มลู
ลําดบั (sequence) เชน่ ขอ้ ความ (text) โดยโมเดลทอี่ อกแบบจะมโี ครงสรา้ งหลกั 3 สว่ น:
1) Embedding Layer - แปลง token id เป็นเวกเตอรเ์ ชงิ ความหมาย
2) LSTM Layer - เรยี นรคู้ วามสมั พันธข์ องคําตามลําดบั เวลา
3) Fully Connected Layer - ใชผ้ ลลพั ธส์ ดุ ทา้ ยของ LSTM ในการจําแนกอารมณ ์
(positive/negative)
การเลอื กใช ้LSTM ทําใหโ้ มเดลสามารถจดจําบรบิ ทของขอ้ ความไดด้ กี วา่ RNN ทวั่ ไป และ
เป็นทางเลอื กทเี่ หมาะสมสําหรับงาน Sentiment Analysis
ขนั้ ตอนท ี่8: ตงั้ คา่ อปุ กรณ ์ สรา้ งโมเดล และฝึกสอน (Training Loop)
ในขนั้ ตอนน ี้ จะเรมิ่ สอนโมเดลทอี่ อกแบบไว ้(LSTMClassifier) ใหเ้รยี นรจู้ ากขอ้ มลู รวี วิ
ภาพยนตร ์โดยจะทําการ
- ตรวจสอบอปุ กรณท์ ใี่ ช ้(GPU / CPU) เพอื่ เรง่ การคํานวณ
- กําหนดคา่ พารามเิ ตอรห์ ลกั เชน่ vocab_size, embedding_dim, hidden_dim และ
output_dim
- ตงั้ คา่ optimizer (ใช ้Adam) และ loss function (ใช ้CrossEntropyLoss สําหรับการ
จําแนก 2 คลาส)
- รัน training loop เพอื่ อปั เดตนํ้าหนักของโมเดลในแตล่ ะ epoch
- แสดงคา่ loss ของแตล่ ะรอบเพอื่ ประเมนิ ความสามารถของโมเดลในการเรยี นร ู้
โมเดลจะคอ่ ย ๆ ปรับคา่ พารามเิ ตอรภ์ ายในผา่ นกระบวนการ backpropagation จนสามารถ
จําแนกรวี วิ ไดแ้ มน่ ยํายงิ่ ขนึ้ ในแตล่ ะ epoch
ขนั้ ตอนท ี่9: ประเมนิ ผลโมเดล (Model Evaluation)
หลงั จากเทรนโมเดลเสร็จแลว้ ขนั้ ตอนนจี้ ะทําการ ทดสอบโมเดลกบั ชดุ ขอ้ มลู ทไี่ มเ่ คยเห็นมา
กอ่ น (test set) เพอื่ ประเมนิ ความสามารถของโมเดลในการจําแนกอารมณข์ องขอ้ ความใหม ่ ๆ
โดยขนั้ ตอนนจี้ ะ
- ตงั้ คา่ โมเดลใหอ้ ยใู่ นโหมด eval() เพอื่ ปิดการอปั เดตคา่ weight
- เกบ็ ผลการทํานาย (preds) และคําตอบจรงิ (labels)
- คํานวณคา่ Accuracy เพอื่ ดวู า่ ผลทํานายถกู ตอ้ งกเี่ ปอรเ์ ซน็ ต ์
- แสดงผลลพั ธส์ ดุ ทา้ ยเป็นคา่ ความแมน่ ยําของโมเดล
ขนั้ ตอนท ี่10: ทดสอบการทํางานของโมเดลกบั ขอ้ ความใหม ่ (Model Inference)

ในขนั้ ตอนสดุ ทา้ ยน ี้ จะนําโมเดลทผี่ า่ นการฝึกสอนแลว้ มาทดลองใชง้ านจรงิ โดยเขยี นฟังกช์ นั
predict_sentiment() เพอื่ ใหผ้ ใู้ ชส้ ามารถพมิ พร์ วี วิ ภาพยนตรไ์ ดเ้อง จากนัน้ โมเดลจะวเิ คราะห์
และทํานายวา่ รวี วิ นัน้ ม ีอารมณบ์ วก (Positive) หรอื อารมณล์ บ (Negative)
ขนั้ ตอนนจี้ งึ เป็นเหมอื นการทดสอบวา่ โมเดลสามารถทํางานไดด้ ใี นสถานการณจ์ รงิ
นอกเหนอื จากชดุ ขอ้ มลู ฝึกหรอื ไม ่
อธบิ ายวธิ ใี นการ train ตวั deep learning network ทเี่ ลอื กมาใช ้รวมไปถงึ อธบิ าย
dataset ทเี่ กยี่ วขอ้ งและแหลง่ ทมี่ า
การ train ตวั Deep Learning Network และ Dataset ทใี่ ช ้
ในโปรเจคนใี้ ช ้LSTM (Long Short-Term Memory) ซงึ่ เป็นโมเดลลําดบั เชงิ ลกึ (Recurrent
Neural Network) ทเี่ หมาะสําหรับขอ้ มลู ประเภทขอ้ ความ เนอื่ งจากสามารถจดจําบรบิ ทระยะ
ยาวในประโยคไดด้ ี
โมเดลถกู train ดว้ ย IMDB Dataset ซงึ่ เป็นชดุ ขอ้ มลู รวี วิ ภาพยนตรภ์ าษาองั กฤษจาก
เว็บไซต ์IMDb ประกอบดว้ ยรวี วิ จํานวน 50,000 ตวั อยา่ ง แบง่ เป็นรวี วิ เชงิ บวกและเชงิ ลบอยา่ ง
ละครงึ่
กอ่ นการเทรน ขอ้ ความถกู ทําความสะอาด (cleaning) และ tokenize ดว้ ย BERT
tokenizer จาก HuggingFace เพอื่ แปลงเป็นตวั เลขทโี่ มเดลสามารถประมวลผลได ้จากนัน้ จงึ
ป้อนเขา้ ส ู่ LSTM model โดยใช ้cross-entropy loss และ Adam optimizer เพอื่ ปรับนํ้าหนัก
โมเดลใหเ้หมาะสม
อธบิ ายการประเมนิ (evaluate) model แสดงคา่ loss จากการ train และ metric ที่
เหมาะสมในการประเมนิ เชน่ accuracy precision หรอื recall
หลงั จากทําการ train โมเดลเสร็จ จะมขี นั้ ตอนการประเมนิ ผล (evaluation) เพอื่ ดวู า่ โม
เดลสามารถจําแนกอารมณข์ องขอ้ ความไดด้ เี พยี งใด โดยใชท้ งั้ คา่ Loss ระหวา่ งการเทรนและ
Accuracy บนชดุ ทดสอบ
1) การแสดงคา่ Loss จากการ Train
ในแตล่ ะรอบ (epoch) จะมกี ารคํานวณ คา่ Loss เพอื่ วดั ความคลาดเคลอื่ นระหวา่ งคา่
ทํานายกบั คา่ จรงิ ยงิ่ loss ลดลง แสดงวา่ โมเดลเรยี นรไู้ ดด้ ขี นึ้
Epoch 1, Loss: 0.6940
Epoch 2, Loss: 0.6890
Epoch 3, Loss: 0.5615
จากตารางจะเห็นวา่ คา่ Loss ลดลงตอ่ เนอื่ ง จาก 0.6940 → 0.5615 แสดงวา่ โมเด
ลมกี ารเรยี นรแู้ ละปรับพารามเิ ตอรไ์ ดอ้ ยา่ งถกู ตอ้ งตามทศิ ทางของขอ้ มลู
2) การวดั ผลดว้ ย Metric ทเี่ หมาะสม
สําหรับปัญหาการจําแนกอารมณ ์ (Sentiment Classification) ซงึ่ เป็นการจําแนกแบบ 2
คลาส (Positive/Negative) ตวั ชวี้ ดั (metric) ทเี่ หมาะสมไดแ้ ก ่ Accuracy วดั สดั สว่ นของผล
ทํานายทถี่ กู ตอ้ งทงั้ หมด
Test Accuracy: 0.80804

หมายความวา่ โมเดลสามารถทํานายอารมณข์ องขอ้ ความรวี วิ ไดถ้ กู ตอ้ งประมาณ 80.8%
ของขอ้ มลู ทงั้ หมด
อธบิ ายบทความอา้ งองิ และงานทเี่ กยี่ วขอ้ ง
งานวจิ ัยดา้ นการจําแนกอารมณข์ องขอ้ ความ (Sentiment Analysis) ไดร้ ับความสนใจ
อยา่ งมากในชว่ งหลายปีทผี่ า่ นมา โดยเรมิ่ จากการใชเ้ทคนคิ Machine Learning แบบดงั้ เดมิ
เชน่ Naïve Bayes และ Support Vector Machine (SVM) รว่ มกบั การแทนขอ้ ความดว้ ย
Bag-of-Words (BoW) หรอื TF-IDF (Pang & Lee, 2008) อยา่ งไรกต็ าม วธิ เี หลา่ นไี้ ม่
สามารถเขา้ ใจบรบิ ทของคําหรอื ความสมั พันธเ์ ชงิ ลําดบั ในประโยคไดด้ นี ัก
ตอ่ มามกี ารพัฒนาเทคนคิ Deep Learning โดยเฉพาะ Recurrent Neural Network
(RNN) และ Long Short-Term Memory (LSTM) ซงึ่ สามารถจดจําขอ้ มลู ตามลําดบั เวลาไดด้ ี
และชว่ ยใหก้ ารจําแนกอารมณข์ องขอ้ ความมคี วามแมน่ ยํามากขนึ้ (Hochreiter &
Schmidhuber, 1997)
ในชว่ งหลงั มกั มกี ารนํา Pre-trained Language Model เชน่ BERT (Bidirectional
Encoder Representations from Transformers) มาใช ้เนอื่ งจากสามารถเรยี นรบู้ รบิ ทของคํา
จากขอ้ ความขนาดใหญแ่ ละเขา้ ใจความหมายไดล้ กึ กวา่ เดมิ (Devlin et al., 2018)
ฐานขอ้ มลู ทนี่ ยิ มใชท้ ดสอบโมเดลในงานลกั ษณะนคี้ อื IMDB Movie Reviews
Dataset ซงึ่ มรี วี วิ ภาพยนตรภ์ าษาองั กฤษจํานวน 50,000 รายการ แบง่ เป็นรวี วิ เชงิ บวกและเชงิ
ลบอยา่ งละเทา่ กนั (Maas et al., 2011) ในโปรเจคนไี้ ดเ้ลอื กใช ้dataset ดงั กลา่ วเพอื่ ฝึกและ
ทดสอบโมเดล LSTM สําหรับจําแนกอารมณข์ องขอ้ ความรวี วิ
Reference
- Pang, B., & Lee, L. (2008). Opinion Mining and Sentiment Analysis.
Foundations and Trends in Information Retrieval.
- Hochreiter, S., & Schmidhuber, J. (1997). Long Short-Term Memory. Neural
Computation.
- Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training
of Deep Bidirectional Transformers for Language Understanding.
- Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., & Potts, C. (2011).
Learning Word Vectors for Sentiment Analysis. ACL.
Dataset
- IMDB movie review sentiment classification dataset
สงิ่ ทตี่ อ้ งสง่ (สง่ มาใน Google Classroom)
● ไฟล ์final_report.pdf
● Github link ทมี่ โี คด้ PyTorch ทเี่ กยี่ วขอ้ งอย ู่
○ ่แปลง final_report.pdf ใหเ้ป็นไฟล ์README.md และ commit ไฟลน์ ลี้ งใน
Github repo เดยี วกนั น ี้

